import numpy as np
from collections import defaultdict, Counter
import pickle
import torch
def binread(filename, shape, dtype_str):
    """
    Reads a binary file and reshapes it to a specified shape.
    shape is a tuple, like (nx*ny, 2), etc.
    dtype_str can be 'int32', 'float32', 'float64', etc.
    """
    count = 1
    for s in shape:
        count *= s
    arr = np.fromfile(filename, dtype=dtype_str, count=count)
    return arr.reshape(shape, order='F')


def read_map(filename, map_shape, precision):
    """
    Used to read spatial mapping files (such as rivlen.bin / rivhgt.bin etc.) and filter based on map_idx.
    map_shape: (nx, ny) or (nx, ny, NLFP), etc.
    When map_shape is 2D, returns data[map_idx]
    When map_shape is 3D, returns data in the form of [(nx*ny), NLFP] and then indexes it.
    """
    if len(map_shape) == 2:
        nx, ny = map_shape
        data = binread(filename, (nx, ny), dtype_str=precision)
    elif len(map_shape) == 3:
        nx, ny, nlfp = map_shape
        # First read into shape [nx*ny, nlfp]
        data = binread(filename, (nx, ny, nlfp), dtype_str=precision)
    else:
        raise ValueError("Unsupported map_shape dimension.")

    return data

def _build_upstream_dict(catchment_ids, next_catchment_ids):
    """
    Builds an inverse adjacency list, optimized version: uses sets and pre-computed dictionaries to accelerate lookups
    """
    catchment_set = set(catchment_ids)
    assert len(catchment_set) == len(catchment_ids), "catchment_ids contains duplicate values!"

    upstream = defaultdict(list)
    for curr, downstream in zip(catchment_ids, next_catchment_ids):
        assert downstream < 0 or downstream in catchment_set
        if downstream in catchment_set:
            upstream[downstream].append(curr)

    return upstream

def _dfs(catchment, upstream, visited):
    """
    Performs DFS post-order traversal starting from catchment as the downstream (outlet),
    ensures all upstream nodes come before downstream nodes, with the rivermouth at the end of the returned list.
    """
    if catchment in visited:
        return []
    visited.add(catchment)
    order = []
    for up in upstream.get(catchment, []):
        order.extend(_dfs(up, upstream, visited))
    order.append(catchment)
    return order

def process_basins(catchment_ids, next_catchment_ids, is_river_mouth):
    """
    Performs DFS traversal on each basin based on the is_river_mouth flag,
    obtaining a topological sort for each basin.

    Returns: List[(rivermouth_id, [topological sequence])]
    """
    upstream = _build_upstream_dict(catchment_ids, next_catchment_ids)
    visited = set()
    basins = []
    rivermouth_ids = catchment_ids[is_river_mouth]
    for rivermouth in rivermouth_ids:
        if rivermouth not in visited:
            order = _dfs(rivermouth, upstream, visited)
            basins.append((rivermouth, order))
    return basins

def verify_basin_integrity(basins, original_catchment_ids, original_next_catchment_ids):
    """
    Verify the integrity of basins generated by process_basins:
    1. Whether basins are mutually independent (no overlapping Catchments).
    2. Whether all original Catchments are covered.
    3. Whether downstream connections within each basin point to catchments within the same basin (or to outlets).

    Args:
        basins: Output of process_basins, formatted as List[(rivermouth_id, [catchment_ids])]
        original_catchment_ids: NumPy array containing all valid catchment IDs input to process_basins.
        original_next_catchment_ids: NumPy array corresponding to original_catchment_ids,
                                     containing the downstream ID for each catchment.

    Returns:
        bool: True if all verifications pass, False otherwise.
    """
    print("\n--- Starting basin integrity verification (independence, coverage, internal links) ---")
    is_independent = True
    covers_all = True
    internal_links_ok = True

    # Preprocessing: Create a mapping from catchment_id to next_catchment_id
    # Ensure input arrays have the same length
    assert len(original_catchment_ids) == len(original_next_catchment_ids), \
        "Original catchment ID and next catchment ID arrays do not match in length!"
    cid_to_next_cid_map = dict(zip(original_catchment_ids, original_next_catchment_ids))
    print(f"Created {len(cid_to_next_cid_map)} mappings from Catchment ID to downstream ID.")

    # Check 1: Whether Catchments overlap across basins
    all_catchments_in_basins = []
    basin_data = [] # Store (rivermouth, set(basin_list))
    for rivermouth, basin_list in basins:
        if not basin_list:
            print(f"Warning: Basin {rivermouth} is empty.")
            continue
        all_catchments_in_basins.extend(basin_list)
        basin_data.append((rivermouth, set(basin_list)))

    catchment_counts = Counter(all_catchments_in_basins)
    overlapping_ids = {cid for cid, count in catchment_counts.items() if count > 1}

    if overlapping_ids:
        print(f"Check 1 failed: Found {len(overlapping_ids)} Catchment IDs that appear in multiple basins!")
        is_independent = False
    else:
        print("Check 1 passed: No Catchment IDs found in multiple basins.")

    # Check 2: Whether all original Catchments are covered
    original_set = set(original_catchment_ids)
    basins_union_set = set(all_catchments_in_basins)

    missing_from_basins = original_set - basins_union_set
    extra_in_basins = basins_union_set - original_set

    if missing_from_basins:
        print(f"Check 2 failed: {len(missing_from_basins)} original Catchment IDs do not appear in any basin!")
        covers_all = False

    if extra_in_basins:
        print(f"Check 2 failed: {len(extra_in_basins)} Catchment IDs appear in basins but not in the original set!")
        covers_all = False

    if not missing_from_basins and not extra_in_basins:
        print("Check 2 passed: The set of Catchment IDs in all basins exactly matches the original set.")

    # Check 3: Whether internal basin connections are valid
    invalid_links_count = 0
    invalid_links_examples = [] # Store (basin_outlet, current_CID, downstream_CID)

    for rivermouth, basin_set in basin_data:
        for current_cid in basin_set:
            # Get downstream ID from the preprocessed mapping
            next_cid = cid_to_next_cid_map.get(current_cid, None) # Use get in case current_cid is not in the mapping

            if next_cid is None:
                 # This shouldn't happen theoretically, as IDs in basin_set should all be in original_catchment_ids
                 print(f"Warning: Catchment ID {current_cid} in basin {rivermouth} not found in the original mapping!")
                 continue

            # Check downstream connection: if the downstream is not negative (i.e., not an outlet or invalid value)
            # and if the downstream ID is not in the current basin's set
            if next_cid >= 0 and next_cid not in basin_set:
                invalid_links_count += 1
                if len(invalid_links_examples) < 10: # Only record a few examples
                    invalid_links_examples.append((rivermouth, current_cid, next_cid))
                internal_links_ok = False

    if not internal_links_ok:
        print(f"Check 3 failed: Found {invalid_links_count} invalid internal downstream connections (pointing outside the basin)!")
        print(f"   Examples of invalid connections (basin outlet, current CID, invalid downstream CID): {invalid_links_examples}")
    else:
        print("Check 3 passed: All downstream connections within basins point to the same basin or to outlets.")


    print("--- Verification complete ---")
    overall_success = is_independent and covers_all and internal_links_ok
    print(f"Final verification result: {'Passed' if overall_success else 'Failed'}")
    return overall_success

def assign_basins_to_gpus(basins, num_gpus=1):
    """
    Assigns each basin to a GPU and returns:
    1. gpu_basin_ids: The order of catchment_ids after merging all basins, without padding
    2. order_index: Maps the original index order, can be used to rearrange other variables
    3. split_indices: Split points for np.split or torch.split_tensor
    """

    # Sort basins by length in descending order
    sorted_basins = sorted(basins, key=lambda x: len(x[1]), reverse=True)

    # Initialize GPU buckets
    gpu_buckets = [[] for _ in range(num_gpus)]
    gpu_loads = [0] * num_gpus

    # Greedily assign basins to the most idle GPU
    for _, basin_order in sorted_basins:
        min_gpu = gpu_loads.index(min(gpu_loads))
        gpu_buckets[min_gpu].extend(basin_order)
        gpu_loads[min_gpu] += len(basin_order)

    # Build gpu_basin_ids (no padding)
    gpu_basin_ids = np.array([id for bucket in gpu_buckets for id in bucket], dtype=np.int64)

    # Calculate split points
    split_indices = []
    cum = 0
    for bucket in gpu_buckets:
        cum += len(bucket)
        split_indices.append(cum)
    # split_indices can be used directly with np.split(arr, split_indices[:-1])

    return gpu_basin_ids, split_indices


def find_indices_in(a, b):
    """
    Returns the index position of each element of a in b.
    Triggers an AssertionError if any elements are not found.
    """

    b_dict = {val: idx for idx, val in enumerate(b)}
    index = np.array([b_dict.get(val, -1) for val in a])

    # assert (index != -1).all(), "Error: Some elements in 'a' are not found in 'b'."

    return index


def compute_runoff_id(runoff_lon, runoff_lat, hires_lon, hires_lat):
    """
    Calculates the runoff grid ID corresponding to each high-resolution grid based on the boundary and spacing of the coarse-resolution grid.
    Triggers an assertion error if out of range.
    """
    gsize_lon = runoff_lon[1] - runoff_lon[0]  # Longitude resolution
    gsize_lat = runoff_lat[0] - runoff_lat[1]  # Latitude resolution (usually negative)

    westin = runoff_lon[0] - 0.5 * gsize_lon
    northin = runoff_lat[0] + 0.5 * gsize_lat

    nxin = len(runoff_lon)
    nyin = len(runoff_lat)

    # Calculate indices (without truncation)
    ixin = np.floor((hires_lon - westin) / gsize_lon).astype(int)
    iyin = np.floor((northin - hires_lat) / gsize_lat).astype(int)

    # Use assert to ensure all indices are within valid range
    assert np.all((ixin >= 0) & (ixin < nxin)), "Some hires_lon points fall outside the runoff grid (longitude)"
    assert np.all((iyin >= 0) & (iyin < nyin)), "Some hires_lat points fall outside the runoff grid (latitude)"

    # Convert 2D grid indices to 1D indices
    runoff_id = iyin * nxin + ixin

    return runoff_id

def save_coo_list_to_pkl(coo_list, filename):
    """
    Save a list of COO sparse tensors to a pickle file.
    
    Args:
        coo_list: List of COO sparse tensors
        filename: Output pickle filename
    """
    serializable_list = []
    for coo in coo_list:
        # Ensure we have a COO tensor
        if not coo.is_sparse or coo.layout != torch.sparse_coo:
            raise ValueError("Input must be a list of COO sparse tensors")
        
        data = {
            'indices': coo.indices().numpy(),
            'values': coo.values().numpy(),
            'shape': coo.shape,
            'dtype': coo.dtype
        }
        serializable_list.append(data)
    
    with open(filename, 'wb') as f:
        pickle.dump(serializable_list, f)

def load_csr_list_from_pkl(filename, device_indices):
    """
    Load sparse tensors from a pickle file as COO format and convert them to CSR format.
    
    Args:
        filename: Input pickle filename
        device_indices: List of device indices to place tensors on
    
    Returns:
        List of CSR sparse tensors
    """
    with open(filename, 'rb') as f:
        serializable_list = pickle.load(f)
    
    csr_list = []
    for data, device in zip(serializable_list, device_indices):
        # First load as COO tensor
        coo = torch.sparse_coo_tensor(
            data['indices'], 
            data['values'], 
            data['shape'], 
            dtype=data['dtype'], 
            device=f"cuda:{device}"
        )
        # Convert to CSR format
        csr = coo.to_sparse_csr()
        csr_list.append(csr)
    
    return csr_list