import numpy as np
from collections import defaultdict, Counter
from CMF_GPU.utils.Variables import MODULES_CONFIG
from CMF_GPU.utils.utils import snapshot_to_pkl, gather_all_keys_and_defaults
from CMF_GPU.utils.Aggregator import generate_triton_aggregator_script
from pathlib import Path
from omegaconf import OmegaConf
import torch
import pickle
import importlib
import os

def binread(filename, shape, dtype_str):
    """
    Reads a binary file and reshapes it to a specified shape.
    shape is a tuple, like (nx*ny, 2), etc.
    dtype_str can be 'int32', 'float32', 'float64', etc.
    """
    count = 1
    for s in shape:
        count *= s
    arr = np.fromfile(filename, dtype=dtype_str, count=count)
    return arr.reshape(shape, order='F')


def read_map(filename, map_shape, precision):
    """
    Used to read spatial mapping files (such as rivlen.bin / rivhgt.bin etc.) and filter based on map_idx.
    map_shape: (nx, ny) or (nx, ny, NLFP), etc.
    When map_shape is 2D, returns data[map_idx]
    When map_shape is 3D, returns data in the form of [(nx*ny), NLFP] and then indexes it.
    """
    if len(map_shape) == 2:
        nx, ny = map_shape
        data = binread(filename, (nx, ny), dtype_str=precision)
    elif len(map_shape) == 3:
        nx, ny, nlfp = map_shape
        # First read into shape [nx*ny, nlfp]
        data = binread(filename, (nx, ny, nlfp), dtype_str=precision)
    else:
        raise ValueError("Unsupported map_shape dimension.")

    return data

def _build_upstream_dict(catchment_ids, next_catchment_ids):
    """
    Builds an inverse adjacency list, optimized version: uses sets and pre-computed dictionaries to accelerate lookups
    """
    catchment_set = set(catchment_ids)
    assert len(catchment_set) == len(catchment_ids), "catchment_ids contains duplicate values!"

    upstream = defaultdict(list)
    for curr, downstream in zip(catchment_ids, next_catchment_ids):
        assert downstream < 0 or downstream in catchment_set
        if downstream in catchment_set:
            upstream[downstream].append(curr)

    return upstream

def _dfs(catchment, upstream, visited):
    """
    Performs DFS post-order traversal starting from catchment as the downstream (outlet),
    ensures all upstream nodes come before downstream nodes, with the rivermouth at the end of the returned list.
    """
    if catchment in visited:
        return []
    visited.add(catchment)
    order = []
    for up in upstream.get(catchment, []):
        order.extend(_dfs(up, upstream, visited))
    order.append(catchment)
    return order

def process_basins(catchment_ids, next_catchment_ids, is_river_mouth):
    """
    Performs DFS traversal on each basin based on the is_river_mouth flag,
    obtaining a topological sort for each basin.

    Returns: List[(rivermouth_id, [topological sequence])]
    """
    upstream = _build_upstream_dict(catchment_ids, next_catchment_ids)
    visited = set()
    basins = []
    rivermouth_ids = catchment_ids[is_river_mouth]
    for rivermouth in rivermouth_ids:
        if rivermouth not in visited:
            order = _dfs(rivermouth, upstream, visited)
            basins.append((rivermouth, order))
    return basins

def verify_basin_integrity(basins, original_catchment_ids, original_next_catchment_ids):
    """
    Verify the integrity of basins generated by process_basins:
    1. Whether basins are mutually independent (no overlapping Catchments).
    2. Whether all original Catchments are covered.
    3. Whether downstream connections within each basin point to catchments within the same basin (or to outlets).

    Args:
        basins: Output of process_basins, formatted as List[(rivermouth_id, [catchment_ids])]
        original_catchment_ids: NumPy array containing all valid catchment IDs input to process_basins.
        original_next_catchment_ids: NumPy array corresponding to original_catchment_ids,
                                     containing the downstream ID for each catchment.

    Returns:
        bool: True if all verifications pass, False otherwise.
    """
    print("\n--- Starting basin integrity verification (independence, coverage, internal links) ---")
    is_independent = True
    covers_all = True
    internal_links_ok = True

    # Preprocessing: Create a mapping from catchment_id to next_catchment_id
    # Ensure input arrays have the same length
    assert len(original_catchment_ids) == len(original_next_catchment_ids), \
        "Original catchment ID and next catchment ID arrays do not match in length!"
    cid_to_next_cid_map = dict(zip(original_catchment_ids, original_next_catchment_ids))
    print(f"Created {len(cid_to_next_cid_map)} mappings from Catchment ID to downstream ID.")

    # Check 1: Whether Catchments overlap across basins
    all_catchments_in_basins = []
    basin_data = [] # Store (rivermouth, set(basin_list))
    for rivermouth, basin_list in basins:
        if not basin_list:
            print(f"Warning: Basin {rivermouth} is empty.")
            continue
        all_catchments_in_basins.extend(basin_list)
        basin_data.append((rivermouth, set(basin_list)))

    catchment_counts = Counter(all_catchments_in_basins)
    overlapping_ids = {cid for cid, count in catchment_counts.items() if count > 1}

    if overlapping_ids:
        print(f"Check 1 failed: Found {len(overlapping_ids)} Catchment IDs that appear in multiple basins!")
        is_independent = False
    else:
        print("Check 1 passed: No Catchment IDs found in multiple basins.")

    # Check 2: Whether all original Catchments are covered
    original_set = set(original_catchment_ids)
    basins_union_set = set(all_catchments_in_basins)

    missing_from_basins = original_set - basins_union_set
    extra_in_basins = basins_union_set - original_set

    if missing_from_basins:
        print(f"Check 2 failed: {len(missing_from_basins)} original Catchment IDs do not appear in any basin!")
        covers_all = False

    if extra_in_basins:
        print(f"Check 2 failed: {len(extra_in_basins)} Catchment IDs appear in basins but not in the original set!")
        covers_all = False

    if not missing_from_basins and not extra_in_basins:
        print("Check 2 passed: The set of Catchment IDs in all basins exactly matches the original set.")

    # Check 3: Whether internal basin connections are valid
    invalid_links_count = 0
    invalid_links_examples = [] # Store (basin_outlet, current_CID, downstream_CID)

    for rivermouth, basin_set in basin_data:
        for current_cid in basin_set:
            # Get downstream ID from the preprocessed mapping
            next_cid = cid_to_next_cid_map.get(current_cid, None) # Use get in case current_cid is not in the mapping

            if next_cid is None:
                 # This shouldn't happen theoretically, as IDs in basin_set should all be in original_catchment_ids
                 print(f"Warning: Catchment ID {current_cid} in basin {rivermouth} not found in the original mapping!")
                 continue

            # Check downstream connection: if the downstream is not negative (i.e., not an outlet or invalid value)
            # and if the downstream ID is not in the current basin's set
            if next_cid >= 0 and next_cid not in basin_set:
                invalid_links_count += 1
                if len(invalid_links_examples) < 10: # Only record a few examples
                    invalid_links_examples.append((rivermouth, current_cid, next_cid))
                internal_links_ok = False

    if not internal_links_ok:
        print(f"Check 3 failed: Found {invalid_links_count} invalid internal downstream connections (pointing outside the basin)!")
        print(f"   Examples of invalid connections (basin outlet, current CID, invalid downstream CID): {invalid_links_examples}")
    else:
        print("Check 3 passed: All downstream connections within basins point to the same basin or to outlets.")


    print("--- Verification complete ---")
    overall_success = is_independent and covers_all and internal_links_ok
    print(f"Final verification result: {'Passed' if overall_success else 'Failed'}")
    return overall_success

def assign_basins_to_gpus(basins, num_gpus=1):
    """
    Assigns each basin to a GPU and returns:
    1. gpu_basin_ids: The order of catchment_ids after merging all basins, without padding
    2. order_index: Maps the original index order, can be used to rearrange other variables
    3. split_indices: Split points for np.split or torch.split_tensor
    """

    # Sort basins by length in descending order
    assert num_gpus > 0, "num_gpus must be greater than 0"
    sorted_basins = sorted(basins, key=lambda x: len(x[1]), reverse=True)

    # Initialize GPU buckets
    gpu_buckets = [[] for _ in range(num_gpus)]
    gpu_loads = [0] * num_gpus

    # Greedily assign basins to the most idle GPU
    for _, basin_order in sorted_basins:
        min_gpu = gpu_loads.index(min(gpu_loads))
        gpu_buckets[min_gpu].extend(basin_order)
        gpu_loads[min_gpu] += len(basin_order)

    # Build gpu_basin_ids (no padding)
    gpu_basin_ids = np.array([id for bucket in gpu_buckets for id in bucket], dtype=np.int64)

    # Calculate split points
    split_indices = []
    cum = 0
    for bucket in gpu_buckets:
        cum += len(bucket)
        split_indices.append(cum)
    # split_indices can be used directly with np.split(arr, split_indices[:-1])

    return gpu_basin_ids, split_indices

def read_bifparam(filename):
    with open(filename, 'r') as f:
        first_line = f.readline().strip().split()
        num_paths = int(first_line[0])
        num_levels = int(first_line[1])

        pth_upst = []
        pth_down = []
        pth_dst = []
        pth_wth = []
        pth_elv = []

        for ipth in range(num_paths):
            line = f.readline().strip().split()
            ix = int(line[0])
            iy = int(line[1])
            jx = int(line[2])
            jy = int(line[3])
            dst = float(line[4])
            pelv = float(line[5])
            pdph = float(line[6])
            
            pth_upst.append([ix, iy])
            pth_down.append([jx, jy])
            pth_dst.append(dst)

            pth_elv_row = []
            pth_wth_row = []
            for ilev in range(num_levels):
                pwth = float(line[7 + ilev])
                if ilev == 0:
                    if pwth > 0:
                        pth_elv_tmp = pelv - pdph
                    else:
                        pth_elv_tmp = 1.0E20
                else:
                    if pwth > 0:
                        pth_elv_tmp = pelv + ilev - 2.0
                    else:
                        pth_elv_tmp = 1.0E20
                pth_elv_row.append(pth_elv_tmp)
                pth_wth_row.append(pwth)
            pth_wth.append(pth_wth_row)
            pth_elv.append(pth_elv_row)

    return num_levels, pth_upst, pth_down, pth_dst, pth_wth, pth_elv

def find_indices_in(a, b):
    """
    Returns the index position of each element of a in b.
    Triggers an AssertionError if any elements are not found.
    """

    b_dict = {val: idx for idx, val in enumerate(b)}
    index = np.array([b_dict.get(val, -1) for val in a])

    # assert (index != -1).all(), "Error: Some elements in 'a' are not found in 'b'."

    return index


def compute_runoff_id(runoff_lon, runoff_lat, hires_lon, hires_lat):
    """
    Calculates the runoff grid ID corresponding to each high-resolution grid based on the boundary and spacing of the coarse-resolution grid.
    Triggers an assertion error if out of range.
    """
    gsize_lon = runoff_lon[1] - runoff_lon[0]  # Longitude resolution
    gsize_lat = runoff_lat[0] - runoff_lat[1]  # Latitude resolution (usually negative)

    westin = runoff_lon[0] - 0.5 * gsize_lon
    northin = runoff_lat[0] + 0.5 * gsize_lat

    nxin = len(runoff_lon)
    nyin = len(runoff_lat)

    # Calculate indices (without truncation)
    ixin = np.floor((hires_lon - westin) / gsize_lon).astype(int)
    iyin = np.floor((northin - hires_lat) / gsize_lat).astype(int)

    # Use assert to ensure all indices are within valid range
    assert np.all((ixin >= 0) & (ixin < nxin)), "Some hires_lon points fall outside the runoff grid (longitude)"
    assert np.all((iyin >= 0) & (iyin < nyin)), "Some hires_lat points fall outside the runoff grid (latitude)"

    # Convert 2D grid indices to 1D indices
    runoff_id = iyin * nxin + ixin

    return runoff_id

def save_coo_list_to_pkl(coo_list, filename):
    """
    Save a list of COO sparse tensors to a pickle file.
    
    Args:
        coo_list: List of COO sparse tensors
        filename: Output pickle filename
    """
    serializable_list = []
    for coo in coo_list:
        # Ensure we have a COO tensor
        if not coo.is_sparse or coo.layout != torch.sparse_coo:
            raise ValueError("Input must be a list of COO sparse tensors")
        
        data = {
            'indices': coo.indices().numpy(),
            'values': coo.values().numpy(),
            'shape': coo.shape,
            'dtype': coo.dtype
        }
        serializable_list.append(data)
    
    with open(filename, 'wb') as f:
        pickle.dump(serializable_list, f)

class DefaultGlobalCatchment:
    num_flood_levels = 10
    
    gravity = 9.81
    river_manning = 0.03 # not used, load from rivman.bin (may be same?)
    flood_manning = 0.1
    river_mouth_distance = 10000.0
    log_buffer_size = 800
    adaptation_factor = 0.7
    missing_value = -9999
    possible_modules = ["base", "adaptive_time_step", "log", "bifurcation"]
    
    var_maps = {
    "river_length": "rivlen.bin",
    "river_width": "rivwth_gwdlr.bin",
    "river_height": "rivhgt.bin",
    "river_manning": "rivman.bin",
    "catchment_elevation": "elevtn.bin",
    "catchment_area": "ctmare.bin",
    "downstream_distance": "nxtdst.bin",
    "flood_depth_table": "fldhgt.bin",
    }
    zero_init_states = [
    "flood_storage",
    "river_outflow",
    "flood_depth",
    "flood_outflow",
    "river_cross_section_depth",
    "flood_cross_section_depth",
    "flood_cross_section_area",
    ]
    map_info = "mapinfo.txt"
    bif_info = "bifprm.txt"

    idx_precision = "<i4"
    map_precision = "<f4"
    hires_map_tag = "1min"
    hires_idx_precision = "<i2"
    hires_map_precision = "<f4"
    numpy_precision = np.float32

    def __init__(self, config):
        self.config = config
        self.map_dir = Path(config.map_dir)
        self.hires_map_dir = Path(config.hires_map_dir)
        self.num_gpus = len(config.runtime_flags["device_indices"])
        self.params = {}
        self.states = {}
        self.reindexed = False
        self.parameters_loaded = False
    
    def _load_map_info(self):
        """
        Loads map dimensions (nx, ny) from the basin.ctl file.
        """
        mapinfo_path = self.map_dir / "basin.ctl"
        with open(mapinfo_path, "r") as f:
            lines = f.readlines()

        # Example assumes nx, ny are on line 3, like in many .ctl files
        for line in lines:
            if line.strip().startswith("xdef"):
                parts = line.strip().split()
                self.nx = int(parts[1])
            elif line.strip().startswith("ydef"):
                parts = line.strip().split()
                self.ny = int(parts[1])
        
        print(f"Loaded map dimensions: nx={self.nx}, ny={self.ny}")


    def _load_catchment_id(self):
        """
        Load catchment IDs and next catchment IDs from binary files.
        """
        nextxy_data = binread(self.map_dir / "nextxy.bin", (self.nx, self.ny, 2), dtype_str=self.idx_precision)
        self.map_shape = nextxy_data.shape[:2]
        catchment_x, catchment_y = np.where(nextxy_data[:, :, 0] != self.missing_value)
        next_catchment_x, next_catchment_y = nextxy_data[catchment_x, catchment_y, 0] - 1, nextxy_data[catchment_x, catchment_y, 1] - 1
        catchment_id = np.ravel_multi_index((catchment_x, catchment_y), self.map_shape)
        next_catchment_id = np.full_like(next_catchment_x, -1, dtype=np.int64)
        valid_next = (next_catchment_x >= 0) & (next_catchment_y >= 0)
        next_catchment_id[valid_next] = np.ravel_multi_index(
            (next_catchment_x[valid_next], next_catchment_y[valid_next]),
            self.map_shape
        )
        self.catchment_x = catchment_x
        self.catchment_y = catchment_y
        self.catchment_id = catchment_id
        self.next_catchment_id = next_catchment_id
        self.is_reservoir = np.zeros_like(catchment_id, dtype=bool)
        self.num_catchments = len(np.where(next_catchment_x != self.missing_value)[0])
        self.is_river_mouth = next_catchment_id < 0

    def _order_basins(self):
        """
        Order basins based on the river mouth and catchment IDs.
        """
        assert not self.reindexed, "order_basins should be called before reindexing!"
        basins = process_basins(self.catchment_id, self.next_catchment_id, self.is_river_mouth)
        verify_basin_integrity(basins, self.catchment_id, self.next_catchment_id)
        assert len(basins) == len(set(rivermouth for rivermouth, _ in basins)), "Duplicate river mouths found!"
        self.gpu_basin_ids, self.split_indices = assign_basins_to_gpus(basins, num_gpus=self.num_gpus)
        self.config.runtime_flags["split_indices"] = self.split_indices

    def _reindex_catchments(self):
        assert not self.reindexed, "Catchments have already been reindexed!"
        loc = find_indices_in(self.gpu_basin_ids, self.catchment_id)
        assert (loc != -1).all()
        self.catchment_x = self.catchment_x[loc]
        self.catchment_y = self.catchment_y[loc]
        self.catchment_id = self.catchment_id[loc]
        self.next_catchment_id = self.next_catchment_id[loc]
        self.downstream_idx = find_indices_in(self.next_catchment_id, self.catchment_id)
        self.is_river_mouth = self.is_river_mouth[loc]
        self.reindexed = True
    
    def _load_parameters(self):
        assert self.reindexed, "load_parameters should be called after reindexing!"
        for param_name, filename in self.var_maps.items():
            file_path = self.map_dir / filename
            if param_name == "flood_depth_table":
                data = read_map(file_path, (self.nx, self.ny, self.num_flood_levels), precision=self.map_precision)[self.catchment_x, self.catchment_y, :]
                data = np.hstack([
                    -self.params["river_height"][:, None],
                    np.zeros((self.num_catchments,1)).astype(self.numpy_precision),
                    data,
                    np.full((self.num_catchments, 1), np.inf).astype(self.numpy_precision)
                    ])
            else:
                data = read_map(file_path, (self.nx, self.ny), precision=self.map_precision)[self.catchment_x, self.catchment_y]
            
            self.params[param_name] = data
        
        self.params["gravity"] = self.gravity
        self.params["is_river_mouth"] = self.is_river_mouth
        self.params["is_reservoir"] = self.is_reservoir
        self.params["downstream_idx"] = self.downstream_idx
        self.params["flood_manning"] = 0.1 * np.ones(self.num_catchments, dtype=self.numpy_precision)
        self.params["log_buffer_size"] = self.log_buffer_size
        self.params["adaptation_factor"] = self.adaptation_factor
        self.params["num_catchments"] = self.num_catchments
        self.params["num_flood_levels"] = self.num_flood_levels
        self.params["downstream_distance"][self.params["is_river_mouth"]] = self.river_mouth_distance
        self.parameters_loaded = True
    
    def _init_river_depth(self):
        """
        Initialize river depth based on the catchment elevation and river height.
        """
        assert self.parameters_loaded, "init_river_depth should be called after load_parameters!"
        river_depth_init = np.zeros(self.num_catchments, dtype=self.numpy_precision)
        next_id_map = find_indices_in(self.next_catchment_id, self.catchment_id)
        river_elevation = self.params["catchment_elevation"] - self.params["river_height"]
        for ii, jj in zip(reversed(range(self.num_catchments)), reversed(next_id_map)):
            if ii == jj or jj < 0:
                river_depth_init[ii] = self.params["river_height"][ii]
            else:
                river_depth_init[ii] = max(
                    river_depth_init[jj] + river_elevation[jj] - river_elevation[ii],
                    0.0
                )
            river_depth_init[ii] = min(river_depth_init[ii], self.params["river_height"][ii])
        self.states["river_depth"] = river_depth_init
        self.states["river_storage"] = self.params["river_width"] * self.states["river_depth"] * self.params["river_length"]
    
    def _simple_init_other_states(self):
        for state in self.zero_init_states:
            self.states[state] = np.zeros(self.num_catchments, dtype=self.numpy_precision)
    
    def _load_bifurcation_parameters(self):
        num_bifurcation_levels, pth_upst, pth_down, pth_dst, pth_wth, pth_elv = read_bifparam(self.map_dir / self.bif_info)
        bifurcation_manning = [self.river_manning] + [self.flood_manning] * (num_bifurcation_levels - 1)
        pth_upst = np.array(pth_upst, dtype=np.int64)
        pth_down = np.array(pth_down, dtype=np.int64)
        bifurcation_catchment_id = np.ravel_multi_index((pth_upst[:, 0] - 1, pth_upst[:, 1] - 1), self.map_shape)
        bifurcation_next_catchment_id = np.ravel_multi_index((pth_down[:, 0] - 1, pth_down[:, 1] - 1), self.map_shape)
        bifurcation_catchment_idx = find_indices_in(bifurcation_catchment_id, self.catchment_id)
        assert (bifurcation_catchment_idx != -1).all(), "Error: Some bifurcation catchment IDs are not found in the main catchment IDs."
        bifurcation_downstream_idx = find_indices_in(bifurcation_next_catchment_id, self.catchment_id)
        assert (bifurcation_downstream_idx != -1).all(), "Error: Some bifurcation downstream IDs are not found in the main catchment IDs."
        num_bifurcation_paths = len(bifurcation_catchment_id)
        self.params["num_bifurcation_paths"] = num_bifurcation_paths
        self.params["num_bifurcation_levels"] = num_bifurcation_levels
        self.params["bifurcation_catchment_idx"] = bifurcation_catchment_idx
        self.params["bifurcation_downstream_idx"] = bifurcation_downstream_idx
        self.params["bifurcation_manning"] = np.tile(np.array(bifurcation_manning, dtype=self.numpy_precision), (num_bifurcation_paths, 1))
        self.params["bifurcation_width"] = np.array(pth_wth, dtype=self.numpy_precision)
        self.params["bifurcation_length"] = np.array(pth_dst, dtype=self.numpy_precision)
        self.params["bifurcation_elevation"] = np.array(pth_elv, dtype=self.numpy_precision)
        self.states["bifurcation_outflow"] = np.zeros((num_bifurcation_paths, num_bifurcation_levels), dtype=self.numpy_precision)
        self.states["bifurcation_cross_section_depth"] = np.zeros((num_bifurcation_paths, num_bifurcation_levels), dtype=self.numpy_precision)

    def _create_input_matrix(self):

        location_file = Path(self.hires_map_dir) / "location.txt"
        with open(location_file, "r") as f:
            lines = f.readlines()

        data = lines[2].split()
        Nx, Ny = int(data[6]), int(data[7])
        West, East = float(data[2]), float(data[3])
        South, North = float(data[4]), float(data[5])
        Csize = float(data[8])
        hires_lon = np.linspace(West  + 0.5 * Csize, East  - 0.5 * Csize, Nx)
        hires_lat = np.linspace(North - 0.5 * Csize, South + 0.5 * Csize, Ny)
        lon2D, lat2D = np.meshgrid(hires_lon, hires_lat)  
        hires_lon_2D = lon2D.T
        hires_lat_2D = lat2D.T

        HighResGridArea = read_map(self.hires_map_dir / f"{self.hires_map_tag}.grdare.bin", (Nx, Ny), precision=self.hires_map_precision) * 1E6
        HighResCatchmentId = read_map(self.hires_map_dir / f"{self.hires_map_tag}.catmxy.bin", (Nx, Ny, 2), precision=self.hires_idx_precision)

        valid_mask = HighResCatchmentId[:, :, 0] > 0
        x_indices, y_indices = np.where(valid_mask)
            
        valid_x = HighResCatchmentId[x_indices, y_indices, 0] - 1  # 1-based to 0-based
        valid_y = HighResCatchmentId[x_indices, y_indices, 1] - 1
        valid_areas = HighResGridArea[x_indices, y_indices]

        catchment_id_hires = np.ravel_multi_index((valid_x, valid_y), (self.nx, self.ny))
        ds_cls = getattr(importlib.import_module("CMF_GPU.utils.Dataset"), self.config.runoff_dataset.class_name)
        example_ds = ds_cls(
            **self.config.runoff_dataset.params
        )
        ro_lon, ro_lat = example_ds.get_coordinates()
        valid_lon = hires_lon_2D[x_indices, y_indices]
        valid_lat = hires_lat_2D[x_indices, y_indices]
        runoff_ids = compute_runoff_id(ro_lon, ro_lat, valid_lon, valid_lat)
        self.runoff_mask = np.zeros((self.nx, self.ny), dtype=np.bool)
        self.runoff_mask[self.catchment_x, self.catchment_y] = True
        
        runoff_mask = example_ds.get_mask()
        runoff_matrix_list = [None] * self.num_gpus
        runoff_mask_list = [np.zeros(len(ro_lat) * len(ro_lon), dtype=np.bool)] * self.num_gpus

        valid_count = 0
        total_count = 0
        for i, gpu_id in enumerate(np.split(self.gpu_basin_ids, self.split_indices[:-1])):
            row_indices = find_indices_in(catchment_id_hires, gpu_id)
            if runoff_mask is not None:
                runoff_mask_row = np.ravel(runoff_mask, order="C")
            else :
                runoff_mask_row = np.ones(len(ro_lat)*len(ro_lon), dtype=bool)
            row_mask = (row_indices != -1) & runoff_mask_row[runoff_ids] # pixels in this gpu and have valid runoff.
            # remap runoff_ids
            unique_ids = np.unique(runoff_ids[row_mask])
            id_map = {old_id: new_id for new_id, old_id in enumerate(unique_ids)}
            remapped_runoff_ids = np.array([id_map[id_val] for id_val in runoff_ids[row_mask]], dtype=np.int64)
            runoff_mask_list[i][unique_ids] = True
            runoff_matrix_list[i] = torch.sparse_coo_tensor(np.vstack((row_indices[row_mask], remapped_runoff_ids)), valid_areas[row_mask], (self.num_catchments, len(unique_ids))).coalesce()
            valid_count += len(np.unique(row_indices[row_mask]))
            total_count += len(np.unique(row_indices[row_indices != -1]))
        if total_count != len(self.gpu_basin_ids):
            print(
                f"Warning: {len(self.gpu_basin_ids) - total_count} catchment(s) could not be mapped to any valid grid cells in the high-resolution map. "
                "These catchments will not receive any runoff input (will always be 0). "
                "If there are many such catchments, this may indicate a mismatch or issue with the input data or mapping logic."
            )
        

        if total_count != valid_count:
            print(
                f"Warning: {total_count - valid_count} catchment(s) will never receive valid runoff data "
                "because all their associated grid cells are invalid; their runoff input will always be 0. "
                "If there are many such catchments, this may indicate an issue with the input data or code logic."
            )

        for i, mat in enumerate(runoff_matrix_list):
            print(f"(GPU{i}) Runoff Input Matrix Shape:", mat.shape)
            print(f"(GPU{i}) Nonzero Elements:", mat._nnz())
        self.runoff_matrix_list = runoff_matrix_list
        self.runoff_mask_list = runoff_mask_list

    def _create_files(self):
        inp_dir = Path(self.config.inp_dir)
        os.makedirs(inp_dir, exist_ok=True)
        save_coo_list_to_pkl(self.runoff_matrix_list, inp_dir / "runoff_input_matrix.pkl")

        snapshot_to_pkl(self.params, "param", self.possible_modules, inp_dir / "parameters.pkl", omit_hidden=True)
        snapshot_to_pkl(self.states, "state", self.possible_modules, inp_dir / "init_states.pkl", omit_hidden=True)

        with open(inp_dir / "runoff_mask.pkl", 'wb') as f:
            pickle.dump(self.runoff_mask_list, f)

        # # update runtime_flags
        # self.config.runtime_flags.update(self.runtime_flags)
        OmegaConf.save(config=self.config, f=inp_dir / "config.yaml")

    def _generate_triton_aggregator_script(self):
        script_path = Path(self.config.inp_dir) / "Aggregate.py"
        agg_keys = self.config["runtime_flags"]["statistics"]
        generate_triton_aggregator_script(script_path, agg_keys)

    def _summary(self):
        # ---------- basic stats ----------
        print(f"Number of GPUs              : {self.num_gpus}")
        print(f"Number of Catchments        : {self.num_catchments}")
        print(f"Number of Bifurcation Paths :  {self.params["num_bifurcation_paths"]}")
        # ---------- 1) completeness check ----------

        errors = []          
        for mod in self.possible_modules:
            cfg       = MODULES_CONFIG.get(mod, {})
            req_p     = set(cfg.get("params", [])) | set(cfg.get("scalar_params", []))
            req_s     = set(cfg.get("states", []))

            missing_p = [p for p in req_p if p not in self.params]
            missing_s = [s for s in req_s if s not in self.states]

            if missing_p:
                errors.append(f"{mod}: missing parameters {missing_p}")
            if missing_s:
                errors.append(f"{mod}: missing states {missing_s}")

        if errors:
            raise ValueError(
                "Required variables are missing:\n  " + "\n  ".join(errors)
            )


        # ---------- 2) modules not enabled ----------
        enabled = set(self.config.runtime_flags.get("modules", []))
        unused_modules = set(self.possible_modules) - enabled
        if unused_modules:
            print(f"\n[INFO] Modules listed in `possible_modules` but NOT enabled for this run: {sorted(unused_modules)}")

        # ---------- 3) extra / unrecognized variables ----------
        # Build the full catalogue of *recognized* variables from all possible modules
        known_p, _, known_scalar = gather_all_keys_and_defaults("param", self.possible_modules)
        known_s, _, _ = gather_all_keys_and_defaults("state", self.possible_modules)
        known_p = set(known_p) | set(known_scalar)
        known_s = set(known_s)

        # Anything not in the catalogue is truly unrecognized â†’ error out
        extra_p = [p for p in self.params if p not in known_p]
        extra_s = [s for s in self.states if s not in known_s]

        extra_errors = []
        if extra_p:
            extra_errors.append(f"unrecognized parameters {sorted(extra_p)}")
        if extra_s:
            extra_errors.append(f"unrecognized states {sorted(extra_s)}")

        if extra_errors:
            raise ValueError(
                "Unrecognized variables detected:\n  " + "\n  ".join(extra_errors)
            )
    
    def build_model_input_pipeline(self):
        """
        Main pipeline to load and process catchment data.
        """
        self._load_map_info()
        self._load_catchment_id()
        self._order_basins()
        self._reindex_catchments()
        self._load_parameters()
        self._init_river_depth()
        self._simple_init_other_states()
        self._load_bifurcation_parameters()
        self._create_input_matrix()
        self._create_files()
        self._generate_triton_aggregator_script()
        self._summary()
        print("Model input pipeline built successfully.")
        
        
if __name__ == "__main__":
    from omegaconf import OmegaConf
    config = OmegaConf.load("./configs/glb_06min.yaml")
    default_global_catchment = DefaultGlobalCatchment(config)
    default_global_catchment.build_model_input_pipeline()