{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from CMF_GPU.utils.Preprocesser import binread, read_map, process_basins,verify_basin_integrity, assign_basins_to_gpus, find_indices_in, compute_runoff_id\n",
    "from omegaconf import OmegaConf\n",
    "config = OmegaConf.load(r\"../configs/test1-glb_15min.yaml\")\n",
    "runtime_flags = config.runtime_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 1440, 720\n",
    "map_dir=config.map_dir\n",
    "nextxy_path = os.path.join(map_dir, \"nextxy.bin\")\n",
    "nextxy_data = binread(nextxy_path, (nx, ny, 2), dtype_str=\"<i4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_x, catchment_y = np.where(nextxy_data[:, :, 0] != -9999)\n",
    "next_catchment_x, next_catchment_y = nextxy_data[catchment_x, catchment_y, 0] - 1, nextxy_data[catchment_x, catchment_y, 1] - 1\n",
    "catchment_id = np.ravel_multi_index((catchment_x, catchment_y), nextxy_data.shape[:2])\n",
    "next_catchment_id = np.full_like(next_catchment_x, -1, dtype=int)\n",
    "valid_next = (next_catchment_x >= 0) & (next_catchment_y >= 0)\n",
    "next_catchment_id[valid_next] = np.ravel_multi_index(\n",
    "    (next_catchment_x[valid_next], next_catchment_y[valid_next]),\n",
    "    nextxy_data.shape[:2]\n",
    ")\n",
    "NSEQMAX = len(np.where(next_catchment_x != -9999)[0])\n",
    "is_river_mouth = next_catchment_id < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topological sorting to get all basins for potential distribution to multiple workers\n",
    "basins = process_basins(catchment_id, next_catchment_id, is_river_mouth)\n",
    "verification_passed = verify_basin_integrity(\n",
    "    basins,\n",
    "    catchment_id,\n",
    "    next_catchment_id\n",
    ")\n",
    "assert len(basins) == len(set(rivermouth for rivermouth, _ in basins)), \"Duplicate river mouths found!\"\n",
    "\n",
    "gpu_basin_ids, split_indices = assign_basins_to_gpus(basins, num_gpus=len(runtime_flags[\"device_indices\"]))\n",
    "\n",
    "runtime_flags[\"split_indices\"] = split_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc =find_indices_in(gpu_basin_ids, catchment_id)\n",
    "assert (loc != -1).all()\n",
    "catchment_x = catchment_x[loc]\n",
    "catchment_y = catchment_y[loc]\n",
    "catchment_id = catchment_id[loc]\n",
    "next_catchment_id = next_catchment_id[loc]\n",
    "is_river_mouth = is_river_mouth[loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VarList=[\"river_length\",\"river_width\",\"river_height\",\"river_manning\",\"catchment_elevation\",\"catchment_area\",\"downstream_distance\"]\n",
    "FileName=[\"rivlen\",\"rivwth_gwdlr\",\"rivhgt\",\"rivman\",\"elevtn\",\"ctmare\",\"nxtdst\",\"fldhgt\"]\n",
    "Precision=\"<f4\"\n",
    "params = {}\n",
    "for var, fname in zip(VarList, FileName):\n",
    "    params[var] = read_map(os.path.join(map_dir, f\"{fname}.bin\"), (nx, ny), precision=Precision)[catchment_x, catchment_y]\n",
    "NLFP = 10\n",
    "flood_depth_table = read_map(os.path.join(map_dir, \"fldhgt.bin\"), (nx, ny, NLFP), precision=Precision)[catchment_x, catchment_y, :]\n",
    "flood_depth_table = np.hstack([\n",
    "    -params[\"river_height\"].reshape(-1, 1),\n",
    "    np.zeros((NSEQMAX,1)).astype(np.float32),\n",
    "    flood_depth_table,\n",
    "    np.full((NSEQMAX, 1), np.inf).astype(np.float32)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize river depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "river_depth_init = np.zeros(NSEQMAX)\n",
    "next_id_map = find_indices_in(next_catchment_id, catchment_id)\n",
    "\n",
    "river_height = params[\"river_height\"]\n",
    "river_length = params[\"river_length\"]\n",
    "river_width = params[\"river_width\"]\n",
    "catchment_area = params[\"catchment_area\"]\n",
    "catchment_elevation = params[\"catchment_elevation\"]\n",
    "downstream_distance = params[\"downstream_distance\"]\n",
    "river_elevation = catchment_elevation - river_height\n",
    "\n",
    "# Initialize river depth (traverse topological sequence from back to front)\n",
    "for ii, jj in zip(reversed(range(NSEQMAX)), reversed(next_id_map)):\n",
    "    if ii == jj or jj < 0:\n",
    "        river_depth_init[ii] = river_height[ii]\n",
    "    else:\n",
    "        river_depth_init[ii] = max(\n",
    "            river_depth_init[jj] + river_elevation[jj] - river_elevation[ii],\n",
    "            0.0\n",
    "        )\n",
    "    river_depth_init[ii] = min(river_depth_init[ii], river_height[ii])\n",
    "river_mouth_distance = 10000.0 \n",
    "downstream_distance[is_river_mouth] = river_mouth_distance\n",
    "river_storage = river_width * river_depth_init * river_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create runoff input matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hires_map_dir = config.hires_map_dir\n",
    "location_file = os.path.join(hires_map_dir, \"location.txt\")\n",
    "\n",
    "with open(location_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data = lines[2].split()\n",
    "Nx, Ny = int(data[6]), int(data[7])\n",
    "West, East = float(data[2]), float(data[3])\n",
    "South, North = float(data[4]), float(data[5])\n",
    "Csize = float(data[8])\n",
    "\n",
    "hires_lon = np.linspace(West  + 0.5 * Csize, East  - 0.5 * Csize, Nx)\n",
    "hires_lat = np.linspace(North - 0.5 * Csize, South + 0.5 * Csize, Ny)\n",
    "lon2D, lat2D = np.meshgrid(hires_lon, hires_lat)  \n",
    "hires_lon_2D = lon2D.T\n",
    "hires_lat_2D = lat2D.T\n",
    "\n",
    "HighResGridArea = read_map(os.path.join(map_dir, hires_map_dir, \"1min.grdare.bin\"), (Nx, Ny), precision=\"<f4\") * 1E6\n",
    "\n",
    "HighResCatchmentId = read_map(os.path.join(map_dir, hires_map_dir, \"1min.catmxy.bin\"), (Nx, Ny, 2), precision=\"<i2\")\n",
    "\n",
    "valid_mask = HighResCatchmentId[:, :, 0] > 0\n",
    "x_indices, y_indices = np.where(valid_mask)\n",
    "    \n",
    "valid_x = HighResCatchmentId[x_indices, y_indices, 0] - 1  # 1-based to 0-based\n",
    "valid_y = HighResCatchmentId[x_indices, y_indices, 1] - 1\n",
    "valid_areas = HighResGridArea[x_indices, y_indices]\n",
    "\n",
    "catchment_id_hires = np.ravel_multi_index((valid_x, valid_y), (nx, ny))\n",
    "\n",
    "ro_lon = np.arange(-179.5, 179.5 + 1, 1)\n",
    "ro_lat = np.arange(89.5, -89.5 - 1, -1)\n",
    "valid_lon = hires_lon_2D[x_indices, y_indices]\n",
    "valid_lat = hires_lat_2D[x_indices, y_indices]\n",
    "\n",
    "runoff_ids = compute_runoff_id(ro_lon, ro_lat, valid_lon, valid_lat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "ds_cls = getattr(importlib.import_module(\"CMF_GPU.utils.Dataloader\"), config.runoff_dataset.class_name)\n",
    "example_ds = ds_cls(\n",
    "    **config.runoff_dataset.params\n",
    ")\n",
    "runoff_mask = example_ds.get_mask()\n",
    "runoff_matrix_list = [None] * len(split_indices) # num_device\n",
    "runoff_mask_list = [np.zeros(len(example_ds.lat) * len(example_ds.lon), dtype=np.bool)] * len(split_indices)\n",
    "\n",
    "valid_count = 0\n",
    "total_count = 0\n",
    "for i, gpu_id in enumerate(np.split(gpu_basin_ids, split_indices[:-1])):\n",
    "    row_indices = find_indices_in(catchment_id_hires, gpu_id)\n",
    "    if runoff_mask is not None:\n",
    "        runoff_mask_row = np.ravel(runoff_mask, order=\"C\")\n",
    "    else :\n",
    "        runoff_mask_row = np.ones(len(example_ds.lat)*len(example_ds.lon), dtype=bool)\n",
    "    row_mask = row_indices != -1 & runoff_mask_row[runoff_ids] # pixels in this gpu and have valid runoff.\n",
    "    # remap runoff_ids\n",
    "    unique_ids = np.unique(runoff_ids[row_mask])\n",
    "    id_map = {old_id: new_id for new_id, old_id in enumerate(unique_ids)}\n",
    "    remapped_runoff_ids = np.array([id_map[id_val] for id_val in runoff_ids[row_mask]], dtype=np.int32)\n",
    "    runoff_mask_list[i][unique_ids] = True\n",
    "    runoff_matrix_list[i] = torch.sparse_coo_tensor(np.vstack((row_indices[row_mask], remapped_runoff_ids)), valid_areas[row_mask], (NSEQMAX, len(unique_ids))).coalesce()\n",
    "    valid_count += len(np.unique(row_indices[row_mask]))\n",
    "    total_count += len(np.unique(row_indices[row_indices != -1]))\n",
    "assert total_count == len(gpu_basin_ids), \"total count mismatch!\"\n",
    "if total_count != valid_count:\n",
    "    print(\n",
    "        f\"Warning: {total_count - valid_count} catchment(s) will never receive valid runoff data \"\n",
    "        \"because all their associated grid cells are invalid; their runoff input will always be 0. \"\n",
    "        \"If there are many such catchments, this may indicate an issue with the input data or code logic.\"\n",
    "    )\n",
    "\n",
    "for i, mat in enumerate(runoff_matrix_list):\n",
    "    print(f\"(GPU{i}) Runoff Input Matrix Shape:\", mat.shape)\n",
    "    print(f\"(GPU{i}) Nonzero Elements:\", mat._nnz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"is_river_mouth\"] = is_river_mouth\n",
    "next_catchment_id[is_river_mouth] = catchment_id[is_river_mouth]\n",
    "downstream_idx = find_indices_in(next_catchment_id, catchment_id)\n",
    "params[\"flood_depth_table\"] = flood_depth_table\n",
    "params[\"downstream_idx\"] = downstream_idx\n",
    "params[\"river_length\"]  = river_length\n",
    "params[\"flood_manning\"] = 0.1 * np.ones(NSEQMAX, dtype=np.float32)\n",
    "params[\"log_buffer_size\"]  = 500\n",
    "params[\"adaptation_factor\"] = 0.7\n",
    "params[\"num_catchments\"] = NSEQMAX\n",
    "params[\"num_flood_levels\"] = 10\n",
    "params[\"gravity\"] = 9.81\n",
    "\n",
    "\n",
    "init_states = {\n",
    "    \"river_storage\": river_storage,\n",
    "    \"river_depth\": river_depth_init,}\n",
    "\n",
    "for state in [\n",
    "    \"flood_storage\",\n",
    "    \"river_outflow\",\n",
    "    \"flood_depth\",\n",
    "    \"flood_outflow\",\n",
    "    \"river_cross_section_depth\",\n",
    "    \"flood_cross_section_depth\",\n",
    "    \"flood_cross_section_area\",\n",
    "]:\n",
    "    init_states[state] = np.zeros(NSEQMAX, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from CMF_GPU.utils.utils import snapshot_to_pkl\n",
    "from CMF_GPU.utils.Preprocesser import save_coo_list_to_pkl\n",
    "inp_dir = config.inp_dir\n",
    "os.makedirs(inp_dir, exist_ok=True)\n",
    "save_coo_list_to_pkl(runoff_matrix_list, os.path.join(inp_dir, \"runoff_input_matrix.pkl\"))\n",
    "\n",
    "snapshot_to_pkl(params, \"param\", runtime_flags[\"modules\"], os.path.join(inp_dir, \"parameters.pkl\"), omit_hidden=True)\n",
    "snapshot_to_pkl(init_states, \"state\", runtime_flags[\"modules\"], os.path.join(inp_dir, \"init_states.pkl\"), omit_hidden=True)\n",
    "\n",
    "with open(os.path.join(inp_dir, \"runoff_mask.pkl\"), 'wb') as f:\n",
    "    pickle.dump(runoff_mask_list, f)    \n",
    "\n",
    "# update runtime_flags\n",
    "config.runtime_flags.update(runtime_flags)\n",
    "OmegaConf.save(config=config, f=os.path.join(inp_dir, \"config.yaml\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# if runoff_mask is not None:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.imshow(runoff_mask, origin='upper')\n",
    "#     plt.show()\n",
    "\n",
    "# runoff_ids_2D = np.full((Ny, Nx), np.nan) \n",
    "# runoff_ids_2D[y_indices, x_indices] = runoff_ids\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.imshow(runoff_ids_2D[1000:5000,2000:7000], cmap='viridis', origin='upper')\n",
    "# plt.title('Runoff IDs on High-Resolution Map')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pythonorder=(np.ravel_multi_index((catchment_y, catchment_x), (ny, nx))+1)\n",
    "# np.savetxt('PythonOrder.csv', pythonorder, delimiter=',', fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
